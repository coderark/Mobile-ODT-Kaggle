{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pickle, math\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from shutil import copyfile\n",
    "from PIL import ImageFile\n",
    "\n",
    "\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Activation, Input\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.metrics import log_loss\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_dir=os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir=cur_dir+'/data/'\n",
    "#data_dir=cur_dir+'/data/sample/'\n",
    "test_dir=cur_dir+'/data/test/'\n",
    "local_model_path=os.path.dirname(cur_dir)+'/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dir=data_dir+'train/'\n",
    "valid_dir=data_dir+'valid/'\n",
    "save_dir=data_dir+'save/'\n",
    "\n",
    "type1_dir=train_dir+'Type_1/'\n",
    "type2_dir=train_dir+'Type_2/'\n",
    "type3_dir=train_dir+'Type_3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_data_dir=data_dir+'sample/'\n",
    "sample_train_dir=sample_data_dir+'train/'\n",
    "sample_valid_dir=sample_data_dir+'valid/'\n",
    "sample_save_dir=sample_data_dir+'save/'\n",
    "\n",
    "sample_type1_dir=sample_train_dir+'Type_1/'\n",
    "sample_type2_dir=sample_train_dir+'Type_2/'\n",
    "sample_type3_dir=sample_train_dir+'Type_3/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel/data\n"
     ]
    }
   ],
   "source": [
    "%cd $data_dir\n",
    "%mkdir -p valid/Type_1\n",
    "%mkdir -p valid/Type_2\n",
    "%mkdir -p valid/Type_3\n",
    "%mkdir results\n",
    "%mkdir save\n",
    "%mkdir -p sample/train/Type_1\n",
    "%mkdir -p sample/train/Type_2\n",
    "%mkdir -p sample/train/Type_3\n",
    "%mkdir -p sample/test\n",
    "%mkdir -p sample/valid/Type_1\n",
    "%mkdir -p sample/valid/Type_2\n",
    "%mkdir -p sample/valid/Type_3\n",
    "%mkdir -p sample/results\n",
    "%mkdir -p sample/save\n",
    "%mkdir -p test/unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = dict()\n",
    "conf['sample_size'] = 0.33\n",
    "conf['val_split'] = 0.2\n",
    "conf['batch_size'] = 64\n",
    "conf['nb_epoch'] = 1\n",
    "conf['patience'] = 3\n",
    "conf['dropout'] = 0.7\n",
    "conf['image_size'] = (224, 224, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel/data/train/Type_1\n"
     ]
    }
   ],
   "source": [
    "%cd $type1_dir\n",
    "g = glob('*.jpg')\n",
    "shuf=np.random.permutation(g)\n",
    "for i in range(int(len(g)*conf['val_split'])): os.rename(shuf[i], valid_dir+ 'Type_1/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel/data/train/Type_2\n"
     ]
    }
   ],
   "source": [
    "%cd $type2_dir\n",
    "g = glob('*.jpg')\n",
    "shuf=np.random.permutation(g)\n",
    "for i in range(int(len(g)*conf['val_split'])): os.rename(shuf[i], valid_dir + 'Type_2/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel/data/train/Type_3\n"
     ]
    }
   ],
   "source": [
    "%cd $type3_dir\n",
    "g = glob('*.jpg')\n",
    "shuf=np.random.permutation(g)\n",
    "for i in range(int(len(g)*conf['val_split'])): os.rename(shuf[i], valid_dir + 'Type_3/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel/data/train/Type_1\n"
     ]
    }
   ],
   "source": [
    "%cd $type1_dir\n",
    "g = glob('*.jpg')\n",
    "shuf=np.random.permutation(g)\n",
    "for i in range(int(len(g)*conf['sample_size'])): copyfile(shuf[i], sample_type1_dir + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel/data/train/Type_2\n"
     ]
    }
   ],
   "source": [
    "%cd $type2_dir\n",
    "g = glob('*.jpg')\n",
    "shuf=np.random.permutation(g)\n",
    "for i in range(int(len(g)*conf['sample_size'])): copyfile(shuf[i], sample_type2_dir + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel/data/train/Type_3\n"
     ]
    }
   ],
   "source": [
    "%cd $type3_dir\n",
    "g = glob('*.jpg')\n",
    "shuf=np.random.permutation(g)\n",
    "for i in range(int(len(g)*conf['sample_size'])): copyfile(shuf[i], sample_type3_dir + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel/data/sample/train/Type_1\n"
     ]
    }
   ],
   "source": [
    "%cd $sample_type1_dir\n",
    "g = glob('*.jpg')\n",
    "shuf=np.random.permutation(g)\n",
    "for i in range(int(len(g)*conf['val_split'])): os.rename(shuf[i], sample_valid_dir+ 'Type_1/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel/data/sample/train/Type_2\n"
     ]
    }
   ],
   "source": [
    "%cd $sample_type2_dir\n",
    "g = glob('*.jpg')\n",
    "shuf=np.random.permutation(g)\n",
    "for i in range(int(len(g)*conf['val_split'])): os.rename(shuf[i], sample_valid_dir+ 'Type_2/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel/data/sample/train/Type_3\n"
     ]
    }
   ],
   "source": [
    "%cd $sample_type3_dir\n",
    "g = glob('*.jpg')\n",
    "shuf=np.random.permutation(g)\n",
    "for i in range(int(len(g)*conf['val_split'])): os.rename(shuf[i], sample_valid_dir+ 'Type_3/' + shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel/data/test\n",
      "mv: cannot stat ‘*.jpg’: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%cd $test_dir\n",
    "%mv *.jpg unknown/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/working/Intel\n"
     ]
    }
   ],
   "source": [
    "%cd $cur_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_preprocess(x):\n",
    "    vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((1,1,3))\n",
    "    x = x - vgg_mean\n",
    "    return x[:, ::-1]\n",
    "\n",
    "def get_batches(dirname, gen=image.ImageDataGenerator(), shuffle=True, batch_size=64, class_mode='categorical',\n",
    "                target_size=(224,224)):\n",
    "    return gen.flow_from_directory(dirname, target_size=target_size,\n",
    "            class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)\n",
    "\n",
    "def get_images(train_dir, claass):\n",
    "    claass_dir = train_dir+claass\n",
    "    images = [claass+'/'+im for im in os.listdir(claass_dir)]\n",
    "    return images\n",
    "\n",
    "def read_image(src, img_size):\n",
    "    im = cv2.imread(src)\n",
    "    im = cv2.resize(im, img_size[0:2], interpolation=cv2.INTER_CUBIC)\n",
    "    return im\n",
    "\n",
    "def load_train(train_dir, img_size):\n",
    "    claasses=[claass for claass in os.listdir(train_dir) if claass[0]!='.']\n",
    "    files = []\n",
    "    y_all = []\n",
    "\n",
    "    for i, claass in enumerate(claasses):\n",
    "        claass_files = get_images(train_dir, claass)\n",
    "        files.extend(claass_files)\n",
    "\n",
    "        y_claass = np.tile(i, len(claass_files))\n",
    "        y_all.extend(y_claass)\n",
    "        print(\"{0} photos of {1}\".format(len(claass_files), claass))\n",
    "    y_all = np.array(y_all)\n",
    "    X_all = np.ndarray((len(files), img_size[0], img_size[1], img_size[2]), dtype=np.uint8)\n",
    "    for i, im in enumerate(files): \n",
    "        #print(train_dir+im)\n",
    "        X_all[i] = read_image(train_dir+im, img_size)\n",
    "        if i%1000 == 0: print('Processed {} of {}'.format(i, len(files)))\n",
    "    print(X_all.shape, y_all.shape)\n",
    "    return X_all, y_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen=image.ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1185 images belonging to 3 classes.\n",
      "Found 296 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "batches_train=get_batches(train_dir, target_size=conf['image_size'][0:2])\n",
    "batches_valid=get_batches(valid_dir, target_size=conf['image_size'][0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels=to_categorical(batches_train.classes)\n",
    "valid_labels=to_categorical(batches_valid.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1185, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_block(model, num_layers, num_filters):\n",
    "    for i in range(num_layers):\n",
    "        model.add(ZeroPadding2D((1, 1)))\n",
    "        model.add(Convolution2D(num_filters, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    return model\n",
    "    \n",
    "def FCB_block(model, units=4096, dropout=False, bn=False):\n",
    "    model.add(Dense(units))\n",
    "    if bn: model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    if dropout: model.add(Dropout(0.5))\n",
    "    return model\n",
    "\n",
    "def create_full_vgg16(input_shape):\n",
    "    model=Sequential()\n",
    "    model.add(Lambda(vgg_preprocess, input_shape=input_shape, output_shape=input_shape))\n",
    "    model=conv_block(model, 2, 64)\n",
    "    model=conv_block(model, 2, 128)    \n",
    "    model=conv_block(model, 3, 256)    \n",
    "    model=conv_block(model, 3, 512)    \n",
    "    model=conv_block(model, 3, 512)\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model=FCB_block(model, dropout=True)\n",
    "    model=FCB_block(model, dropout=True)\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "    fname='vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "    local_path=local_model_path+fname\n",
    "    down_path='https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "    if os.path.exists(local_path):\n",
    "        model.load_weights(local_path)\n",
    "    else:\n",
    "        model.load_weights(get_file(fname, down_path, cache_subdir='models'))\n",
    "    return model\n",
    "\n",
    "def create_conv_vgg16(input_shape):\n",
    "    model=Sequential()\n",
    "    model.add(ZeroPadding2D((1, 1), input_shape=input_shape))\n",
    "    model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "    model=conv_block(model, 1, 64)\n",
    "    model=conv_block(model, 2, 128)    \n",
    "    model=conv_block(model, 3, 256)    \n",
    "    model=conv_block(model, 3, 512)    \n",
    "    model=conv_block(model, 3, 512)\n",
    "    fname='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    local_path=local_model_path+fname\n",
    "    down_path='https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "    if os.path.exists(local_path):\n",
    "        model.load_weights(local_path)\n",
    "    else:\n",
    "        model.load_weights(get_file(fname, down_path, cache_subdir='models'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def finetune(model, num_class, level=2):\n",
    "    layers=model.layers\n",
    "    layers_idx=[idx for idx, layer in enumerate(layers) if type(layer) is Convolution2D or type(layer) is Dense]\n",
    "    for i in range(layers_idx[level*-1]): layers[i].trainable=False\n",
    "    model.pop()\n",
    "    model.add(Dense(num_class, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def get_base_model(model, level=2):\n",
    "    layers=model.layers\n",
    "    layers_idx=[idx for idx, layer in enumerate(layers) if type(layer) is Convolution2D or type(layer) is Dense]\n",
    "    for i in range(len(layers)): \n",
    "        if i>=layers_idx[level*-1]:\n",
    "            model.pop()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16_full=create_full_vgg16(conf['image_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16_ft1=finetune(vgg16_full, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16_base1=get_base_model(vgg16_ft1, level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 226, 226, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 226, 226, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 114, 114, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 114, 114, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 58, 58, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 58, 58, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 30, 30, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 30, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_base1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `predict_generator` not found.\n"
     ]
    }
   ],
   "source": [
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 606, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/preprocessing/image.py\", line 727, in __next__\n",
      "    return self.next(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/preprocessing/image.py\", line 960, in next\n",
      "    target_size=self.target_size)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/keras/preprocessing/image.py\", line 330, in load_img\n",
      "    img = img.resize(wh_tuple)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/Image.py\", line 1630, in resize\n",
      "    self.load()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/PIL/ImageFile.py\", line 231, in load\n",
      "    \"(%d bytes not processed)\" % len(b))\n",
      "OSError: image file is truncated (54 bytes not processed)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape () for Tensor 'lambda_2_input:0', which has shape '(?, 224, 224, 3)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-19be9ead85f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvgg16_base1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalid_base\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvgg16_base1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, max_q_size, workers, pickle_safe, verbose)\u001b[0m\n\u001b[1;32m   1178\u001b[0m                                             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m                                             \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m                                             verbose=verbose)\n\u001b[0m\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, steps, max_q_size, workers, pickle_safe, verbose)\u001b[0m\n\u001b[1;32m   2093\u001b[0m                     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2095\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2096\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2097\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1682\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_predict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2103\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2104\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 786\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    787\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    970\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    973\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape () for Tensor 'lambda_2_input:0', which has shape '(?, 224, 224, 3)'"
     ]
    }
   ],
   "source": [
    "train_base=vgg16_base1.predict_generator(batches_train, math.ceil(batches_train.n/conf['batch_size']))\n",
    "valid_base=vgg16_base1.predict_generator(batches_valid, math.ceil(batches_valid.n/conf['batch_size']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(save_dir+'X_train_conv_224', train_base)\n",
    "np.save(save_dir+'X_valid_conv_224', valid_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_base=np.load(save_dir+'X1_train_conv_224.npy')\n",
    "valid_base=np.load(save_dir+'X1_valid_conv_224.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1185, 25088)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_base.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16_top1=Sequential()\n",
    "vgg16_top1.add(Dense(4096, input_shape=(25088, )))\n",
    "vgg16_top1.add(BatchNormalization())\n",
    "vgg16_top1.add(Activation('relu'))\n",
    "vgg16_top1.add(Dropout(conf['dropout']))\n",
    "vgg16_top1.add(Dense(4096))\n",
    "vgg16_top1.add(BatchNormalization())\n",
    "vgg16_top1.add(Activation('relu'))\n",
    "vgg16_top1.add(Dropout(conf['dropout']))\n",
    "vgg16_top1.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=1e-4, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg16_top1.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1185 samples, validate on 296 samples\n",
      "Epoch 1/5\n",
      "1185/1185 [==============================] - 36s - loss: 0.0598 - acc: 0.9958 - val_loss: 1.3358 - val_acc: 0.4696\n",
      "Epoch 2/5\n",
      "1185/1185 [==============================] - 23s - loss: 0.0556 - acc: 0.9958 - val_loss: 1.3308 - val_acc: 0.4628\n",
      "Epoch 3/5\n",
      "1185/1185 [==============================] - 22s - loss: 0.0604 - acc: 0.9958 - val_loss: 1.3295 - val_acc: 0.4662\n",
      "Epoch 4/5\n",
      "1185/1185 [==============================] - 23s - loss: 0.0647 - acc: 0.9949 - val_loss: 1.3301 - val_acc: 0.4662\n",
      "Epoch 5/5\n",
      "1185/1185 [==============================] - 23s - loss: 0.0578 - acc: 0.9966 - val_loss: 1.3325 - val_acc: 0.4662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f69000b3cc0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16_top1.fit(train_base, train_labels, batch_size=conf['batch_size'], epochs=5, validation_data=(valid_base, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_top1.save_weights(save_dir+'m1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
